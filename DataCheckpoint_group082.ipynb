{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Samuel\n",
    "- Matthew\n",
    "- Caitlin\n",
    "- Darren\n",
    "- Nick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do large online communities ( of retail traders) have influence over the stock market?\n",
    "\n",
    "*Sentiment analysis of positivity on the Reddit subreddit r/WallStreetBets and how this correlates to the performance of the S&P 500 from January 31, 2012 to the present.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stock Dataset\n",
    "\n",
    "- Dataset Name: Stock Market S&P 500 History \n",
    "- Link to the dataset: https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC\n",
    "- Number of observations: 2,273\n",
    "\n",
    "This dataset contains the date, open, high, low, close, adjusted close, and volume of the S&P 500 from January 31, 2012 to February 10, 2021. We got this dataset from Yahoo Finance, which allows us to easily download the history of the S&P 500 into a CSV file. We chose this time period as the subreddit r/wallstreetbets was created on January 31, 2012. In our data cleaning code, we will only keep the date and closing columns.\n",
    "\n",
    "\n",
    "\n",
    "#### Reddit Dataset\n",
    "\n",
    "- Dataset Name: Wallstreetbets Subs Full\n",
    "- Link to the dataset: https://drive.google.com/file/d/1l3NuVbJtf9mdMdvLsKRnj0rcfYYp7o28/view?usp=sharing\n",
    "- Number of observations: 1,317,200\n",
    "\n",
    "Our team found a dataset on Kaggle that gave us the submissions on r/wallstreetbets in a dataset that went up to August 2020. To ensure we covered the entire period within the scope of our question we elected to acquire our own data using wrappers for the Reddit API. We have included our webscraping code below. We webscraped the Reddit API from January 31, 2012 (when the subreddit was created) to the present. This dataset contains submissions to the subreddit. Other columns include features in Reddit (awards, removals, etc.) as well as links to posts and authors. We will be cleaning this up to better organize the data by date and post content.\n",
    "\n",
    "*Our stock dataset is included in the GitHub folder.*\n",
    "\n",
    "*Our reddit dataset is provided in a Google Drive link as the file is 1.7 gb.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what we need for dataset paragraph:\n",
    "\n",
    "\n",
    "DATASET(S): What data will you use to answer your question? Describe the \n",
    "dataset(s) in terms of number of observations, what kind of features it \n",
    "contains, etc. (Typically students have datasets of ~1000 observations \n",
    "across their datasets. This is not a requirement, but is good to know around \n",
    "the scale of data we're expecting.) You are welcome (and in fact recommended) \n",
    "to find multiple datasets! If you do so, describe each one, and briefly \n",
    "explain how you will combine them together. Include the source of the dataset \n",
    "in the description here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports\n",
    "#!pip install pmaw\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stock Market S&P 500 History \n",
    "\n",
    "Our question is associated with how the positivity of the subreddit r/wallstreetbets correlates to the performance of the S&P 500 from January 31, 2012 to the present. This dataset before cleaning is already very clean. We just need to remove certain columns in order to get what we need to answer our research question. Therefore, we will just need the date and the closing price of the S&P 500. We do not need the adjusted close due to that we are working with the S&P 500 and don't need to work with out of hours like we would with an individual stock or the the opening price since it will just be the previous day's closing price. Since we are only considering the performance of the S&P 500, we do not need the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/31/2012</td>\n",
       "      <td>1312.410034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/1/2012</td>\n",
       "      <td>1324.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2/2/2012</td>\n",
       "      <td>1325.540039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2/3/2012</td>\n",
       "      <td>1344.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/6/2012</td>\n",
       "      <td>1344.329956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>2/4/2021</td>\n",
       "      <td>3871.739990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>2/5/2021</td>\n",
       "      <td>3886.830078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>2/8/2021</td>\n",
       "      <td>3915.590088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>2/9/2021</td>\n",
       "      <td>3911.229980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>2/10/2021</td>\n",
       "      <td>3909.879883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2273 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Close\n",
       "0     1/31/2012  1312.410034\n",
       "1      2/1/2012  1324.089966\n",
       "2      2/2/2012  1325.540039\n",
       "3      2/3/2012  1344.900024\n",
       "4      2/6/2012  1344.329956\n",
       "...         ...          ...\n",
       "2268   2/4/2021  3871.739990\n",
       "2269   2/5/2021  3886.830078\n",
       "2270   2/8/2021  3915.590088\n",
       "2271   2/9/2021  3911.229980\n",
       "2272  2/10/2021  3909.879883\n",
       "\n",
       "[2273 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = pd.read_csv(\"Stock_Market_S&P_500_History.csv\")\n",
    "cleaned_stocks = stocks[['Date', 'Close']]\n",
    "cleaned_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reddit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to webscrape Reddit API\n",
    "\n",
    "This was used outside of our notebook in order to webscrape the Reddit API for the subreddit r/wallstreetbets from January 31, 2012 to February 12, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import os\n",
    "\n",
    "outname = 'wallstreetbets_subs_full.csv'\n",
    "\n",
    "outdir = './data'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "fullname = os.path.join(outdir, outname)\n",
    "\n",
    "api = PushshiftAPI()\n",
    "submissions = api.search_submissions(subreddit=\"wallstreetbets\", after=1327968000, before=1613160000)\n",
    "\n",
    "sub_df = pd.DataFrame(submissions)\n",
    "sub_df.to_csv(fullname, header=True, index=False, columns=list(sub_df.axes[1]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wallstreetbets Subs Full\n",
    "\n",
    "We webscraped the subreddit r/wallstreetbets for a consistent time period. The Kaggle dataset we found stopped at August 2020. We wanted a time period that would span from when the subreddit was created (January 31, 2012) to the present. Our dataset, when first webscraped, has many unnecessary columns. Considering how our research question only asks about the positivity of the subreddit, columns such as 'subreddit', and 'event_is_live' are unneeded. We will be focusing on the following columns: ________________________________ . Some columns are associated with Reddit features that do not pertain to our research question. We also further cleaned our dataset by renaming some columns for better understanding. Such columns include changing the name from ________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fff36b49c4f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreddit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wallstreetbets_subs_full.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# The below code was run on a local machine due to size constraints of our CSV\n",
    "\n",
    "reddit = pd.read_csv(\"./wallstreetbets_subs_full.csv\", low_memory = False)\n",
    "reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns to get a better sense of what data is where, and what we know we don't need\n",
    "\n",
    "collist = list(reddit)\n",
    "print(collist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all posts are from one subreddit\n",
    "\n",
    "allinsubreddit = sum(reddit['subreddit'] != 'wallstreetbets')\n",
    "print(allinsubreddit)\n",
    "\n",
    "# Yes they are, dropping the redundant column\n",
    "reddit.drop('subreddit', axis=1)\n",
    "\n",
    "# Also dropping 3 columns with information unrelated to our scope\n",
    "reddit.drop(['event_end','event_is_live','event_start'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a few columns we know we need for clarity\n",
    "\n",
    "reddit.rename(columns={'author':'Author', 'author_fullname':'Author ID', 'title':'Post Title'}, inplace=True)\n",
    "reddit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal (updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/11  | 9 PM  | Finalising our initial look at chosen databases | Finish creating checkpoint\n",
    "| 2/12  | Before 11:59 PM  | NA | Turn in Checkpoint #1: Data |\n",
    "| 2/15  | 8 PM  | Start data wrangling beyond initial setup | Start exploring EDA \n",
    "| 2/17  | 8 PM  | Finalize wrangling/EDA; Begin Analysis /// | Discuss/edit Analysis |\n",
    "| 2/22  | 8 PM  | Work on individual tasks | Review all work thus far, finish and submit checkpoint 2   |\n",
    "| 2/26  | Before 11:59 PM  | NA | Turn in Checkpoint #2: EDA  |\n",
    "| 3/3  | 8 PM  | Start to look at final project deliverable | Begin finishing analysis |\n",
    "| 3/8  | 8 PM  | Complete analysis; Draft results/conclusion/discussion /// | Work on and discuss the final deliverable |\n",
    "| 3/19 [??]  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
